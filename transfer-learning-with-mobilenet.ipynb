{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "channels = 3\n",
    "img_size = 32\n",
    "\n",
    "data = pd.read_csv(\"../input/digit-recognizer/train.csv\")\n",
    "data.label = data.label.astype('category')\n",
    "\n",
    "mobile = MobileNet(include_top=False, weights='imagenet', input_shape=(img_size,img_size,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = data[data.label==0][:32]\n",
    "for i in range(1,10):\n",
    "    valset = pd.concat([valset, data[data.label==i][:32]], axis=0)\n",
    "data = data.drop(valset.index, axis=0)\n",
    "\n",
    "subset = data[data.label==0][:32]\n",
    "for i in range(1,10):\n",
    "    subset = pd.concat([subset, data[data.label==i][:32]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = subset.label\n",
    "y_val = valset.label\n",
    "subset = subset.drop(columns=['label'])\n",
    "valset = valset.drop(columns=['label'])\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "subset = np.reshape(np.array(subset), newshape=(32*10,28,28))\n",
    "valset = np.reshape(np.array(valset), newshape=(32*10,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_resize(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        img = Image.fromarray(data[i].astype('uint8'), 'L')\n",
    "        img = img.convert('RGB').resize((32,32), Image.ANTIALIAS)\n",
    "        new_data.append(np.array(img))\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to 0-1\n",
    "new_subset = convert_resize(subset)/255\n",
    "new_valset = convert_resize(valset)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trfModel():\n",
    "    model = Sequential()\n",
    "    model.add(mobile)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 9s 27ms/step - loss: 2.4462 - acc: 0.3688\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 1.0270 - acc: 0.7250\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.6501 - acc: 0.8000\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4273 - acc: 0.8594\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2774 - acc: 0.9344\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3035 - acc: 0.9125\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3023 - acc: 0.9094\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2681 - acc: 0.9250\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.1969 - acc: 0.9281\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.1854 - acc: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efff13f6ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trfModel()\n",
    "model.fit(x=new_subset, y=y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9111291021108627, 0.809375]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=new_valset, y=encoder.transform(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_y_train = encoder.transform(data.label)\n",
    "data = data.drop(columns=['label'])\n",
    "data = np.reshape(np.array(data), newshape=(len(data), 28, 28))\n",
    "\n",
    "full_x_train = convert_resize(data)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41680/41680 [==============================] - 53s 1ms/step - loss: 0.2995 - acc: 0.9233\n",
      "Epoch 2/10\n",
      "41680/41680 [==============================] - 48s 1ms/step - loss: 0.1233 - acc: 0.9685\n",
      "Epoch 3/10\n",
      "41680/41680 [==============================] - 47s 1ms/step - loss: 0.1192 - acc: 0.9716\n",
      "Epoch 4/10\n",
      "41680/41680 [==============================] - 47s 1ms/step - loss: 0.1673 - acc: 0.9614\n",
      "Epoch 5/10\n",
      "41680/41680 [==============================] - 47s 1ms/step - loss: 0.2535 - acc: 0.9478\n",
      "Epoch 6/10\n",
      "41680/41680 [==============================] - 46s 1ms/step - loss: 0.2667 - acc: 0.9532\n",
      "Epoch 7/10\n",
      "41680/41680 [==============================] - 46s 1ms/step - loss: 0.0985 - acc: 0.9763\n",
      "Epoch 8/10\n",
      "41680/41680 [==============================] - 47s 1ms/step - loss: 0.0644 - acc: 0.9849\n",
      "Epoch 9/10\n",
      "41632/41680 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9819"
     ]
    }
   ],
   "source": [
    "full_model = trfModel()\n",
    "full_model.fit(x=full_x_train, y=full_y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18138439753092825, 0.95625]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.evaluate(x=new_valset, y=encoder.transform(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN + Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model(\"../input/conditional-dcgan-cnn-using-32-images/generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = []\n",
    "for i in range(10):\n",
    "    condition = np.zeros(10, dtype='int')\n",
    "    condition[i] = 1\n",
    "    conditions.append([condition for z in range(3200)])\n",
    "\n",
    "conditions = np.reshape(np.array(conditions), newshape=(3200*10,10))\n",
    "noises = np.random.normal(0,1, size=(3200*10,100))\n",
    "synthetic_images = generator.predict([noises, conditions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_images = synthetic_images*255\n",
    "synthetic_images = np.reshape(synthetic_images, newshape=(3200*10, 28, 28))\n",
    "reshaped_synthetic_images = convert_resize(synthetic_images)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 43s 1ms/step - loss: 0.1319 - acc: 0.9807\n",
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0459 - acc: 0.9936\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0349 - acc: 0.9937\n",
      "Epoch 4/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 5/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0145 - acc: 0.9966\n",
      "Epoch 6/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0271 - acc: 0.9944\n",
      "Epoch 7/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0239 - acc: 0.9951\n",
      "Epoch 8/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 9/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0050 - acc: 0.9983\n",
      "Epoch 10/10\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 0.0223 - acc: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f001dd742e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_model = trfModel()\n",
    "synthetic_model.fit(x=reshaped_synthetic_images, y=conditions, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 2s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2700042124837637, 0.91875]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_model.evaluate(x=new_valset, y=encoder.transform(y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
